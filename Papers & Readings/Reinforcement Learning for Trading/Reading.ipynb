{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作者用强化学习训练交易系统，提出了一种强化学习方法称为递归强化学习（Recurrent Reinforcement Learning， RRL），并提出了一种值函数称为差分夏普率（Differential Sharpe Ratio），并在1970年到1994年的标普500指数（S&P 500 index）的月线数据上与一些方法的做了对比试验，证实了标普500指数的可预测性，同时取得了较好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 记号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交易指令"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑某一合约的价值序列$z_t$，交易员（Trader）可以执行买多、持有、卖多指令，记为$F_t\\in\\{-1, 0, 1\\}$，交易指令执行的手数为一个常数$\\mu$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交易损耗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从$t$到$t+1$时刻，对两个交易指令$F_t$与$F_{t+1}$，交易系统（Trading System）的奖励函数值（Rewards）记作$R_{t+1}$将由价差收益与交易损耗构成，将交易损耗记为$\\delta$，交易损耗由手续费、佣金、税费产生。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 递归强化学习（Recurrent Reinforcement Learning， RRL）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为交易员定义内部状态，并且递归的定义这个内部状态，可以更好地将系统性风险、交易损耗等纳入交易指令的执行考虑，所以交易员的决策函数将具有以下形式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_t=F(\\theta_t; F_{t-1}, I_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_t$是决策函数待学习的参数，$I_t$是$t$时刻的已知信息，包含了过去的价格序列，和其他外部指定的变量："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I=\\{z_t, z_{t-1}, z_{t-2}, \\ldots; y_t, y_{t-1}, y_{t-2}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 附加利润 （Additive Profits）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_T=\\sum^{T}_{t=1}R_t=\\mu\\sum^{T}_{t=1}\\{r^f_t+F_{t-1}(r_t-r^f_t)-\\delta \\left| F_t-F_{t-1}\\right|\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$r_t=z_t-z_{t-1}$$\n",
    "$$r^f_t=z^f_t-z^f_{t-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个公式中，$\\mu$是每一个交易指令作用在某单个合约的手数，$r_t$与$r^f_t$是当前合约与基线合约的价差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 乘数利润（Multiplicative profits）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W_T=W_0\\prod^{T}_{t=1}\\{1+R_t\\}=W_0\\prod^{T}_{t=1}\\{1+(1-F_{t-1})r^f_t+F_{t-1}r_t\\}\\{1-\\delta \\left|F_t-F_{t-1}\\right|\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$r_t=(\\frac{z_t}{z_{t-1}}-1)$$\n",
    "$$r^f_t=(\\frac{z^f_t}{z^f_{t-1}}-1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个公式中，相对于附加利润，$r_t$与$r^f_t$的形式被改写为分数形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 差分夏普率（Differential Sharpe Ratio）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而不同于希望最大化上两个公式，对冲基金希望最大化的是夏普比率："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_T = \\frac{Average(R_t)}{Standard Deviation(R_t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "差分夏普率记作："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D_t\\equiv\\frac{dS_t}{d\\mu}=\\frac{B_{t-1}\\Delta A_t-\\frac{1}{2}A_{t-1}\\Delta{B_t}}{(B_{t-1}-A^2_{t-1})^{\\frac{3}{2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A_t = A_{t-1}+\\mu \\Delta A_t = A_{t-1} + \\mu(R_t-A_{t_1})$$\n",
    "$$B_t = B_{t-1}+\\mu \\Delta B_t = B_{t-1} + \\mu(R^2_t-B_{t_1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 值函数序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值序列$U_T=U(R_1, R_2, \\ldots, R_T)$，其梯度可以通过如下公式计算："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dU_T(\\theta)}{d\\theta}=\\sum^{T}_{t=1}\\frac{U_T}{dR_t}\\{\\frac{dR_t}{dF_t}\\frac{dF_t}{d\\theta}+\\frac{dR_t}{dF_{t-1}}\\frac{dF_{t-1}}{d\\theta}\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于在线学习（On-line Learning）对于$t$时刻，其梯度可以通过如下公式计算："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dU_t(\\theta)}{d\\theta}=\\frac{U_t}{dR_t}\\{\\frac{dR_t}{dF_t}\\frac{dF_t}{d\\theta}+\\frac{dR_t}{dF_{t-1}}\\frac{dF_{t-1}}{d\\theta}\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后通过反向传播算法与随机梯度下降更新权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图是一个实验结果，其中黑色实线是递归强化学习配合最大化差分夏普率的表现，相对于买入和持有策略与Qtrader策略，RRL策略取得了较好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure-1](./Figure-1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
